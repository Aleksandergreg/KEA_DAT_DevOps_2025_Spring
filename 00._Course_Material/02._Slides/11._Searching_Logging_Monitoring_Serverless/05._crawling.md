<div class="title-card">
    <h1>Web crawling - Where to run the script?</h1>
</div>

---

# Running the script on a server

Not recommended because of:

**Competing ressources**: If we run it on the same server as our production VM.

**Cost**: If we run provision a dedicated server just to run the script.

---

# Running the script locally

Create a script that can be run manually. 

Let it scrape from the internet and upload to the production database via the backend.

**Recommendation**: When uploading, create a route that allows batch inserts to the database. Since the endpoint would be public, create a key to authenticate the request.

---

# Github Actions (Cron Schedule)

You can run a GitHub action workflow in intervals (not triggered by actions in the repository).

However, there are major downsides:

> Jobs may be dropped during high load times. Avoid this by scheduling at odd hours.

> In a public repository, scheduled workflows are automatically disabled when no repository activity has occurred in 60 days. For information on re-enabling a disabled workflow, see "Disabling and enabling a workflow."

https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows#schedule

---

# Github Actions Cron Schedule - Example


```yaml
name: Run Scrapy Project Daily

on:
  schedule:
    - cron: '0 0 * * *'  # Runs at midnight (UTC) every day

jobs:
  run-scrapy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Scrapy Spider
        run: |
          scrapy crawl <spider_name> -o output.json

```

---

# In a Serverless Function

This would be the optimal solution in a modern company.

You only pay for the time the serverless function is running.

You don't have to deal with installing things to get the runtime working. 

---

# How often should your script run?

This depends on what the users are searching for.  

* **Once**: If the data is not changing.

* **At regular intervals**: Keep overwriting rows / adding new rows.

---

<div class="title-card">
    <h1>Hands-on crawling - Running it locally</h1>
</div>

---

# Let's set up a small project

Set up a project and install jsdom that will help us parse HTML:

```bash
$ npm init -y
$ npm install jsdom
```

Remember to change `"type": "module"` in your `package.json` to use ES modules.

Create a file called `crawler.js` and paste the code on the next slide into it. Run it with:

```bash
$ node crawler.js
```

---

# Crawler Example

```javascript
import { JSDOM } from 'jsdom';
import { URL } from 'url';

const visitedUrls = new Set();

function delay(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
}

async function crawlPage(pageUrl) {
    const cleanUrl = new URL(pageUrl);
    cleanUrl.hash = '';

    if (visitedUrls.has(cleanUrl.href)) {
        return;
    }

    visitedUrls.add(cleanUrl.href);

    try {
        const response = await fetch(cleanUrl.href);
        if (!response.ok) {
            console.error(`Failed to fetch ${cleanUrl.href}: ${response.statusText}`);
            return;
        }

        const html = await response.text();
        const { window } = new JSDOM(html);
        const document = window.document;

        const mainContent = Array.from(document.querySelectorAll('.mw-parser-output p, .mw-parser-output h1, .mw-parser-output h2, .mw-parser-output h3'))
            .map(element => element.textContent.trim())
            .join(' ')
            .replace(/[\t\n\r]+/g, ' ')
            .trim();

        console.log('Indexing:', cleanUrl.href);
        // todo send or save cleanUrl.href, mainContent somewhere

        const links = Array.from(document.querySelectorAll('a')).map(link => link.getAttribute('href'));
        for (let href of links) {
            if (href && href.startsWith('/wiki/') && !href.includes(':')) {
                const resolvedUrl = new URL(href, cleanUrl.origin).href;
                const normalizedUrl = resolvedUrl.split('#')[0];
                
                if (!visitedUrls.has(normalizedUrl)) {
                    await delay(1000);
                    await crawlPage(normalizedUrl);
                }
            }
        }
    } catch (error) {
        console.error(`Error crawling ${cleanUrl.href}:`, error);
    }
}

crawlPage('https://en.wikipedia.org/wiki/Elasticsearch');
```